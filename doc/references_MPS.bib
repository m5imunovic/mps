@article{Lecun1998,
   author = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
   doi = {10.1109/5.726791},
   issn = {00189219},
   issue = {11},
   journal = {Proceedings of the IEEE},
   pages = {2278-2324},
   title = {Gradient-based learning applied to document recognition},
   volume = {86},
   year = {1998},
}
@inproceedings{Deng2009,
   author = {Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei},
   doi = {10.1109/CVPR.2009.5206848},
   isbn = {978-1-4244-3992-8},
   journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
   month = {6},
   pages = {248-255},
   publisher = {IEEE},
   title = {ImageNet: A large-scale hierarchical image database},
   year = {2009},
}
@article{Graves2013,
   abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
   author = {Alex Graves},
   month = {8},
   title = {Generating Sequences With Recurrent Neural Networks},
   year = {2013},
}
@inproceedings{Cho2014,
   author = {Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
   city = {Stroudsburg, PA, USA},
   doi = {10.3115/v1/D14-1179},
   journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
   pages = {1724-1734},
   publisher = {Association for Computational Linguistics},
   title = {Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
   year = {2014},
}
@article{Krueger2003,
   abstract = {<p>In this work, the emphasis is on the development of strategies to realize techniques of numerical computing on the graphics chip. In particular, the focus is on the acceleration of techniques for solving sets of algebraic equations as they occur in numerical simulation. We introduce a framework for the implementation of linear algebra operators on programmable graphics processors (GPUs), thus providing the building blocks for the design of more complex numerical algorithms. In particular, we propose a stream model for arithmetic operations on vectors and matrices that exploits the intrinsic parallelism and efficient communication on modern GPUs. Besides performance gains due to improved numerical computations, graphics algorithms benefit from this model in that the transfer of computation results to the graphics processor for display is avoided. We demonstrate the effectiveness of our approach by implementing direct solvers for sparse matrices, and by applying these solvers to multi-dimensional finite difference equations, i.e. the 2D wave equation and the incompressible Navier-Stokes equations.</p>},
   author = {Jens Krüger and Rüdiger Westermann},
   doi = {10.1145/882262.882363},
   issn = {0730-0301},
   issue = {3},
   journal = {ACM Transactions on Graphics},
   month = {7},
   pages = {908-916},
   title = {Linear algebra operators for GPU implementation of numerical algorithms},
   volume = {22},
   year = {2003},
}
@article{Hornik1989,
   author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
   doi = {10.1016/0893-6080(89)90020-8},
   issn = {08936080},
   issue = {5},
   journal = {Neural Networks},
   month = {1},
   pages = {359-366},
   title = {Multilayer feedforward networks are universal approximators},
   volume = {2},
   year = {1989},
}
@article{Chetlur2014,
   abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption.},
   author = {Sharan Chetlur and Cliff Woolley and Philippe Vandermersch and Jonathan Cohen and John Tran and Bryan Catanzaro and Evan Shelhamer},
   month = {10},
   title = {cuDNN: Efficient Primitives for Deep Learning},
   year = {2014},
}
@article{Blackford2002,
   author = {L.S. Blackford},
   doi = {10.1145/567806.567807},
   issn = {0098-3500},
   issue = {2},
   journal = {ACM Transactions on Mathematical Software},
   month = {6},
   pages = {135-151},
   title = {An updated set of basic linear algebra subprograms (BLAS)},
   volume = {28},
   year = {2002},
}
@article{Rumelhart1986,
   author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
   doi = {10.1038/323533a0},
   issn = {0028-0836},
   issue = {6088},
   journal = {Nature},
   month = {10},
   pages = {533-536},
   title = {Learning representations by back-propagating errors},
   volume = {323},
   year = {1986},
}
@article{Hochreiter1997,
   abstract = {<p>Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.</p>},
   author = {Sepp Hochreiter and Jürgen Schmidhuber},
   doi = {10.1162/neco.1997.9.8.1735},
   issn = {0899-7667},
   issue = {8},
   journal = {Neural Computation},
   month = {11},
   pages = {1735-1780},
   title = {Long Short-Term Memory},
   volume = {9},
   year = {1997},
}
@article{Bahdanau2014,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
   month = {9},
   title = {Neural Machine Translation by Jointly Learning to Align and Translate},
   year = {2014},
}
@inproceedings{Krizhevsky2012,
   author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E Hinton},
   editor = {F Pereira and C J Burges and L Bottou and K Q Weinberger},
   journal = {Advances in Neural Information Processing Systems},
   publisher = {Curran Associates, Inc.},
   title = {ImageNet Classification with Deep Convolutional Neural Networks},
   volume = {25},
   url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
   year = {2012},
}
@inproceedings{Vaswani2017,
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Ł ukasz Kaiser and Illia Polosukhin},
   editor = {I Guyon and U Von Luxburg and S Bengio and H Wallach and R Fergus and S Vishwanathan and R Garnett},
   journal = {Advances in Neural Information Processing Systems},
   publisher = {Curran Associates, Inc.},
   title = {Attention is All you Need},
   volume = {30},
   url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
   year = {2017},
}

@InProceedings{Glorot2010,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
}
@article{Ioffe2015,
  author       = {Sergey Ioffe and
                  Christian Szegedy},
  title        = {Batch Normalization: Accelerating Deep Network Training by Reducing
                  Internal Covariate Shift},
  journal      = {CoRR},
  volume       = {abs/1502.03167},
  year         = {2015},
  url          = {http://arxiv.org/abs/1502.03167},
  eprinttype    = {arXiv},
  eprint       = {1502.03167},
  timestamp    = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/IoffeS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Srivastava2014,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    url={http://www.deeplearningbook.org},
    year={2016}
}
@misc{Kingma2017,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{Dao2022,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{Dao2023,
      title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}, 
      author={Tri Dao},
      year={2023},
      eprint={2307.08691},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}